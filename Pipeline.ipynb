{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for visualise the default number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualise_default():\n",
    "    year_list = ['2004', '2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
    "    data_ = []\n",
    "    default = []\n",
    "    total = []\n",
    "    for year_ in year_list:\n",
    "        print(\"Loading data...\")\n",
    "        data_path = 'data_flag/{year}_flag.csv'.format(year=year_)  \n",
    "        cols = pd.read_csv(data_path).columns\n",
    "        data = pd.read_csv(data_path, usecols = cols[1:]) \n",
    "        y = np.asarray(data['default_flag'].astype(int))\n",
    "        u_ele, ct_ele = np.unique(y, return_counts=True)\n",
    "        default.append(ct_ele[1])\n",
    "        total.append(np.size(y))\n",
    "    data_.append(default)\n",
    "    data_.append(total)\n",
    "    columns = ['%d' % x for x in np.arange(2004,2019)]\n",
    "    rows = ['default','number of accounts'] \n",
    "    fig, ax = plt.subplots(figsize=(30, 20))\n",
    "    values = np.arange(0, 2500, 500)\n",
    "    colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n",
    "    n_rows = len(data_)\n",
    "\n",
    "    index = np.arange(len(columns)) + 0.6\n",
    "    bar_width = 0.4\n",
    "\n",
    "    # Initialize the vertical-offset for the stacked bar chart.\n",
    "    y_offset = np.zeros(len(columns))\n",
    "\n",
    "    # Plot bars and create text labels for the table\n",
    "    for row in range(n_rows):\n",
    "        bar = plt.bar(index, data_[row], bar_width, bottom=y_offset, color=colors[row])\n",
    "        y_offset = y_offset + data_[row]\n",
    "        cell_text.append(['%d' % x for x in y_offset])\n",
    "    \n",
    "    colors = colors[::1]\n",
    "\n",
    "    # Add a table at the bottom of the axes\n",
    "    the_table = plt.table(cellText=data_,\n",
    "                          rowLabels=rows,\n",
    "                          rowColours=colors,\n",
    "                          colLabels=columns,\n",
    "                          loc='bottom')\n",
    "    the_table.set_fontsize(25)\n",
    "    the_table.scale(1.0, 2.0)  # may help\n",
    "\n",
    "    # Adjust layout to make room for the table:\n",
    "    values = np.arange(0, 2000000, 500000)\n",
    "    plt.ylabel(\"Person\",fontsize = 20)\n",
    "    plt.yticks(values, ['%d' % val for val in values], fontsize = 20)\n",
    "    plt.xticks([])\n",
    "    plt.title('Default counts by years',fontsize = 20)\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('./Defaults.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, data_path_time):\n",
    "    time_start=time.time()\n",
    "    \n",
    "    #Load data\n",
    "    data_list = []\n",
    "    for fname in sorted(os.listdir(data_path)):\n",
    "        subject_data_path = os.path.join(data_path, fname)\n",
    "        print(subject_data_path)\n",
    "        \n",
    "        if not os.path.isfile(subject_data_path): continue\n",
    "        data_list.append(\n",
    "            pd.read_csv(\n",
    "                        subject_data_path,\n",
    "                        sep='|', \n",
    "                        header=None,\n",
    "                        names = [\n",
    "                                'CREDIT_SCORE',\n",
    "                                'FIRST_PAYMENT_DATE',\n",
    "                                'FIRST_TIME_HOMEBUYER_FLAG',\n",
    "                                '4','5','6',\n",
    "                                'NUMBER_OF_UNITS',\n",
    "                                'OCCUPANCY_STATUS',\n",
    "                                '9',\n",
    "                                'ORIGINAL_DTI_RATIO',\n",
    "                                'ORIGINAL_UPB',\n",
    "                                'ORIGINAL_LTV',\n",
    "                                'ORIGINAL_INTEREST_RATE',\n",
    "                                'CHANNEL',\n",
    "                                '15',\n",
    "                                'PRODUCT_TYPE',\n",
    "                                'PROPERTY_STATE',\n",
    "                                'PROPERTY_TYPE',\n",
    "                                '19',\n",
    "                                'LOAN_SQ_NUMBER',\n",
    "                                'LOAN_PURPOSE',\n",
    "                                'ORIGINAL_LOAN_TERM',\n",
    "                                'NUMBER_OF_BORROWERS',\n",
    "                                '24','25','26'#,'27'#data from every year may have different column number\n",
    "                            #2004-2007: 27 2008: 26 2009: 27\n",
    "                        ],\n",
    "                        usecols=[\n",
    "                            'CREDIT_SCORE',\n",
    "                            'FIRST_TIME_HOMEBUYER_FLAG',\n",
    "                            'NUMBER_OF_UNITS',\n",
    "                            'OCCUPANCY_STATUS',\n",
    "                            'ORIGINAL_DTI_RATIO',\n",
    "                            'ORIGINAL_UPB',\n",
    "                            'ORIGINAL_LTV',\n",
    "                            'ORIGINAL_INTEREST_RATE',\n",
    "                            'CHANNEL',\n",
    "                            'PROPERTY_TYPE',\n",
    "                            'LOAN_SQ_NUMBER',\n",
    "                            'LOAN_PURPOSE',\n",
    "                            'ORIGINAL_LOAN_TERM',\n",
    "                            'NUMBER_OF_BORROWERS'\n",
    "                        ],\n",
    "                        dtype={'CREDIT_SCORE':np.float_, \n",
    "                               'FIRST_TIME_HOMEBUYER_FLAG':np.str, \n",
    "                               'NUMBER_OF_UNITS':np.int_, \n",
    "                               'OCCUPANCY_STATUS':np.str,\n",
    "                               'ORIGINAL_DTI_RATIO':np.float_,\n",
    "                               'ORIGINAL_UPB':np.float_,\n",
    "                               'ORIGINAL_LTV':np.float_,\n",
    "                               'ORIGINAL_INTEREST_RATE':np.float_,\n",
    "                               'CHANNEL':np.str,\n",
    "                               'PROPERTY_TYPE':np.str,\n",
    "                               'LOAN_SQ_NUMBER':np.str,\n",
    "                               'LOAN_PURPOSE':np.str,\n",
    "                               'ORIGINAL_LOAN_TERM':np.int_,\n",
    "                               'NUMBER_OF_BORROWERS':np.int_},\n",
    "                        low_memory=False\n",
    "                        )\n",
    "        )\n",
    "    data = pd.concat(data_list)\n",
    "    \n",
    "    #Load data with time\n",
    "    data_p_list=[]\n",
    "    for fname in sorted(os.listdir(data_path_time)):\n",
    "        subject_data_path = os.path.join(data_path_time, fname)\n",
    "        print(subject_data_path)\n",
    "        if not os.path.isfile(subject_data_path): continue\n",
    "        data_p_list.append(\n",
    "            pd.read_csv(subject_data_path,\n",
    "                             sep='|',\n",
    "                             header=None,\n",
    "                             usecols=[0,3,4],\n",
    "                             dtype={'0':np.str, '3':np.str, '4':np.int_}\n",
    "                            )\n",
    "        )\n",
    "    #data_p = pd.concat(data_p_list)\n",
    "    \n",
    "    #Calculate default\n",
    "    default_list=[]\n",
    "    for data_p in data_p_list:\n",
    "        data_p[3] = data_p[3].astype(str)\n",
    "        clean_index = data_p.iloc[:,1].str.isdigit()\n",
    "        data_p_cleaned = data_p[clean_index].copy()\n",
    "        data_p_cleaned[3] = data_p_cleaned[3].astype(int)\n",
    "        data_less_than_48 = data_p_cleaned[data_p_cleaned[4] < 48]\n",
    "        default_list.append(data_less_than_48[data_less_than_48[3] > 2])\n",
    "    \n",
    "    data_default = pd.concat(default_list)\n",
    "    \n",
    "    default_index = data['LOAN_SQ_NUMBER'].isin(data_default[0].tolist())\n",
    "    \n",
    "    \n",
    "    \n",
    "    data['default_flag']=default_index\n",
    "    \n",
    "    \n",
    "    data.drop(columns=['LOAN_SQ_NUMBER'], inplace=True)\n",
    "    #data.to_csv('data/historical_data_withflag.csv',index=False)\n",
    "    \n",
    "    #Imputation\n",
    "    CREDIT_SCORE = data['CREDIT_SCORE']\n",
    "    OIR = data['ORIGINAL_DTI_RATIO']\n",
    "    LTV = data['ORIGINAL_LTV']\n",
    "    CREDIT_clean = CREDIT_SCORE[CREDIT_SCORE != 9999]\n",
    "    OIR_clean = OIR[OIR != 999]\n",
    "    LTV_clean = LTV[LTV != 999]\n",
    "    data['CREDIT_SCORE'] = data['CREDIT_SCORE'].apply(lambda x : CREDIT_clean.mean() if x == 9999 else x)\n",
    "    data['ORIGINAL_DTI_RATIO'] = data['ORIGINAL_DTI_RATIO'].apply(lambda x : OIR_clean.mean() if x == 999 else x)\n",
    "    data['ORIGINAL_LTV'] = data['ORIGINAL_LTV'].apply(lambda x : LTV_clean.mean() if x == 999 else x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Timer stop\n",
    "    time_end=time.time()\n",
    "    print('Finished loading, time cost:',time_end-time_start,'s')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data_standardize():\n",
    "    years = ['2004', '2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "    cols = pd.read_csv('data_flag/2004_flag.csv').columns\n",
    "    data = pd.DataFrame(columns = cols[1:].append(pd.Index(['YEAR']))) \n",
    "    for year_ in years:\n",
    "        data_path = 'data_flag/{year}_flag.csv'.format(year=year_)  \n",
    "        cols = pd.read_csv(data_path).columns\n",
    "        data_this_year = pd.read_csv(data_path, usecols = cols[1:]) \n",
    "        data_this_year['YEAR'] = year_\n",
    "        data = data.append(data_this_year, ignore_index=True)\n",
    "\n",
    "\n",
    "    year_list = data['YEAR']\n",
    "    data.drop(columns=['YEAR'])\n",
    "\n",
    "    data['FIRST_TIME_HOMEBUYER_FLAG'] = data['FIRST_TIME_HOMEBUYER_FLAG'].apply(lambda x : np.NaN if x == '9' else x)\n",
    "    data['NUMBER_OF_UNITS'] = data['NUMBER_OF_UNITS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "    data['CHANNEL'] = data['CHANNEL'].apply(lambda x : np.NaN if x == 'T' else x)\n",
    "    data['PROPERTY_TYPE'] = data['PROPERTY_TYPE'].apply(lambda x : np.NaN if x == '99' else x)\n",
    "    data['NUMBER_OF_BORROWERS'] = data['NUMBER_OF_BORROWERS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "\n",
    "\n",
    "    output_array = np.asarray(data['default_flag'].astype(int))\n",
    "\n",
    "    input_values = np.c_[\n",
    "        data[['CREDIT_SCORE',#0\n",
    "              'ORIGINAL_DTI_RATIO',#1\n",
    "              'ORIGINAL_UPB',#2\n",
    "              'ORIGINAL_LTV',#3\n",
    "              'ORIGINAL_LOAN_TERM',#4\n",
    "              'ORIGINAL_INTEREST_RATE'#5\n",
    "             ]]\n",
    "    ] \n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(input_values)\n",
    "    input_values = scaler.transform(input_values)\n",
    "\n",
    "    input_dummies = np.c_[\n",
    "        np.asarray(pd.get_dummies(data['FIRST_TIME_HOMEBUYER_FLAG'])), # N,Y,9 str remove 9\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_UNITS'])), # 1 2 3 4 99 int remove 99\n",
    "        np.asarray(pd.get_dummies(data['OCCUPANCY_STATUS'])), # P S I str\n",
    "        np.asarray(pd.get_dummies(data['CHANNEL'])), # T R C B str remove T\n",
    "        np.asarray(pd.get_dummies(data['PROPERTY_TYPE'])), # SF PU CO MH CP 99 str remove 99\n",
    "        np.asarray(pd.get_dummies(data['LOAN_PURPOSE'])), # P N C str\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_BORROWERS'])) # 2 1 99 int remove 99\n",
    "    ]\n",
    "\n",
    "    input_array = np.c_[\n",
    "        input_values,\n",
    "        input_dummies\n",
    "    ]\n",
    "\n",
    "    data_stand = pd.DataFrame(input_array)\n",
    "    data_stand['YEAR'] = year_list\n",
    "\n",
    "    output = pd.DataFrame(output_array)\n",
    "    output['YEAR'] = year_list\n",
    "\n",
    "    for year_ in years:\n",
    "        print('Preparing {year} ...'.format(year=year_))\n",
    "        data_path = 'data_train/{year}/'.format(year=year_)\n",
    "        data_this_year = data_stand.loc[data_stand['YEAR'] == year_]\n",
    "        data_this_year = data_this_year.drop(columns=['YEAR'])\n",
    "        output_this_year = output.loc[output['YEAR'] == year_]\n",
    "        output_this_year = output_this_year.drop(columns=['YEAR'])\n",
    "        folder = os.getcwd() + '/data_train/{year}/'.format(year=year_)\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        X_path = data_path + 'input.csv'\n",
    "        y_path = data_path + 'output.csv'\n",
    "        data_this_year.to_csv(X_path.format(year_))\n",
    "        output_this_year.to_csv(y_path.format(year_))\n",
    "        print('{year} done!'.format(year=year_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_log_N_S(data, fig, ax, penalty):\n",
    "   \n",
    "    time_start=time.time()\n",
    "    #Get dummy value\n",
    "   \n",
    "    \n",
    "    data['FIRST_TIME_HOMEBUYER_FLAG'] = data['FIRST_TIME_HOMEBUYER_FLAG'].apply(lambda x : np.NaN if x == '9' else x)\n",
    "    data['NUMBER_OF_UNITS'] = data['NUMBER_OF_UNITS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "    data['CHANNEL'] = data['CHANNEL'].apply(lambda x : np.NaN if x == 'T' else x)\n",
    "    data['PROPERTY_TYPE'] = data['PROPERTY_TYPE'].apply(lambda x : np.NaN if x == '99' else x)\n",
    "    data['NUMBER_OF_BORROWERS'] = data['NUMBER_OF_BORROWERS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "    \n",
    "    \n",
    "    output_array = np.asarray(data['default_flag'].astype(int))\n",
    "    input_array = np.c_[\n",
    "        data[['CREDIT_SCORE',#0\n",
    "              'ORIGINAL_DTI_RATIO',#1\n",
    "              'ORIGINAL_UPB',#2\n",
    "              'ORIGINAL_LTV',#3\n",
    "              'ORIGINAL_LOAN_TERM',#4\n",
    "              'ORIGINAL_INTEREST_RATE'#5\n",
    "             ]],\n",
    "        np.asarray(pd.get_dummies(data['FIRST_TIME_HOMEBUYER_FLAG'])), # N,Y,9 str remove 9\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_UNITS'])), # 1 2 3 4 99 int remove 99\n",
    "        np.asarray(pd.get_dummies(data['OCCUPANCY_STATUS'])), # P S I str\n",
    "        np.asarray(pd.get_dummies(data['CHANNEL'])), # T R C B str remove T\n",
    "        np.asarray(pd.get_dummies(data['PROPERTY_TYPE'])), # SF PU CO MH CP 99 str remove 99\n",
    "        np.asarray(pd.get_dummies(data['LOAN_PURPOSE'])), # P N C str\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_BORROWERS'])) # 2 1 99 int remove 99\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    X = input_array \n",
    "    y = output_array\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        random_state=13\n",
    "    )\n",
    "    #Normalise\n",
    "#     scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "#     X_train = scaler.transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    \n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     X_train = min_max_scaler.fit_transform(X_train)\n",
    "#     X_test = min_max_scaler.transform(X_test)\n",
    "    \n",
    "    if penalty == 1:\n",
    "        classifier = LogisticRegression(\n",
    "            tol= 1e-6,\n",
    "            C=0.05,\n",
    "            max_iter = 500,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    else:\n",
    "        classifier = LogisticRegression(\n",
    "            penalty = 'none',\n",
    "            tol= 1e-6,\n",
    "            #C=0.05,\n",
    "            #class_weight = 'balanced',\n",
    "            #class_weight = {0:0.01, 1:0.99},\n",
    "            #solver='sag',\n",
    "            max_iter=500,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "#     viz = plot_roc_curve(\n",
    "#         classifier, \n",
    "#         X_test, \n",
    "#         y_test,\n",
    "#         name='Test ROC'.format(0),\n",
    "#         alpha=0.5, lw=1, ax=ax\n",
    "#     )\n",
    "    \n",
    "#     viz_train = plot_roc_curve(\n",
    "#         classifier, \n",
    "#         X_train, \n",
    "#         y_train,\n",
    "#         name='Train ROC'.format(1),\n",
    "#         alpha=0.5, lw=1, ax=ax\n",
    "#     ) \n",
    "    \n",
    "#     ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "#         label='Chance', alpha=.8)\n",
    "    \n",
    "#     ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "#        title=\"Receiver operating characteristic example\")\n",
    "#     ax.legend(loc=\"lower right\")\n",
    "    time_end=time.time()\n",
    "    print('Training done, time cost:',time_end-time_start,'s') \n",
    "    #print(classifier.predict_proba(X_test)[:, 1])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_log_Y_S(X, y, fig, ax, penalty):\n",
    "   \n",
    "    time_start=time.time()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        random_state=13\n",
    "    )\n",
    "    \n",
    "    if penalty == 1:\n",
    "        classifier = LogisticRegression(\n",
    "            tol= 1e-5,\n",
    "            C=0.05,\n",
    "            max_iter=500,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    else:\n",
    "        classifier = LogisticRegression(\n",
    "            penalty = 'none',\n",
    "            tol= 1e-5,\n",
    "            #class_weight = 'balanced',\n",
    "            #class_weight = {0:0.01, 1:0.99},\n",
    "            #solver='sag',\n",
    "            max_iter=500,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "#     viz = plot_roc_curve(\n",
    "#         classifier, \n",
    "#         X_test, \n",
    "#         y_test,\n",
    "#         name='Test ROC'.format(0),\n",
    "#         alpha=0.5, lw=1, ax=ax\n",
    "#     )\n",
    "    \n",
    "#     viz_train = plot_roc_curve(\n",
    "#         classifier, \n",
    "#         X_train, \n",
    "#         y_train,\n",
    "#         name='Train ROC'.format(1),\n",
    "#         alpha=0.5, lw=1, ax=ax\n",
    "#     ) \n",
    "    \n",
    "    time_end=time.time()\n",
    "    print('Training done, time cost:',time_end-time_start,'s') \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_log_diff(X, y, c):\n",
    "   \n",
    "    time_start=time.time()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        random_state=13\n",
    "    )\n",
    "    \n",
    "    classifier = LogisticRegression(\n",
    "        penalty = 'l2',\n",
    "        tol= 1e-5,\n",
    "        C=c,\n",
    "        #class_weight = 'balanced',\n",
    "        #class_weight = {0:0.01, 1:0.99},\n",
    "        #solver='sag',\n",
    "        max_iter=1500,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "    time_end=time.time()\n",
    "    print('Training done, time cost:',time_end-time_start,'s')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_log_cross_validation(data):\n",
    "    #Get dummy value\n",
    "    output_array = np.asarray(data['default_flag'].astype(int))\n",
    "    input_array = np.c_[\n",
    "        data[['CREDIT_SCORE',#0\n",
    "              'ORIGINAL_DTI_RATIO',#1\n",
    "              'ORIGINAL_UPB',#2\n",
    "              'ORIGINAL_LTV',#3\n",
    "              'ORIGINAL_LOAN_TERM',#4\n",
    "              'ORIGINAL_INTEREST_RATE'#5\n",
    "             ]],\n",
    "        np.asarray(pd.get_dummies(data['FIRST_TIME_HOMEBUYER_FLAG'])),\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_UNITS'])),\n",
    "        np.asarray(pd.get_dummies(data['OCCUPANCY_STATUS'])),\n",
    "        np.asarray(pd.get_dummies(data['CHANNEL'])),\n",
    "        np.asarray(pd.get_dummies(data['PROPERTY_TYPE'])),\n",
    "        np.asarray(pd.get_dummies(data['LOAN_PURPOSE'])),\n",
    "        np.asarray(pd.get_dummies(data['NUMBER_OF_BORROWERS']))\n",
    "    ]\n",
    "    \n",
    "    #Normalise\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    input_array_N = min_max_scaler.fit_transform(input_array)\n",
    "    \n",
    "    X = input_array_N\n",
    "    y = output_array\n",
    "    \n",
    "    #devide Flods for cv\n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    #define classifier\n",
    "    classifier = LogisticRegression(\n",
    "        solver='saga',\n",
    "        max_iter=1500\n",
    "    )\n",
    "\n",
    "    #tpr lists and auc value list\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    \n",
    "    #For ploting, prepare 500 points from 0-1\n",
    "    mean_fpr = np.linspace(0, 1, 500)\n",
    "      \n",
    "    #Loop training for every fold\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X[train], y[train])\n",
    "        # put the curve in ax through 'ax = ax'\n",
    "        viz = plot_roc_curve(classifier, X[test], y[test],\n",
    "                             name='ROC fold {}'.format(i),\n",
    "                             alpha=0.3, lw=1, ax=ax)\n",
    "\n",
    "        #Plot every point (500) form 0-1, similiar to bin\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        #Buff the result \n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    #Plot chance\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    #mean value for each colomn\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    \n",
    "    std_auc = np.std(aucs)\n",
    "    #Plot mean\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    #Plot standard tpr (Doesn't know the point for this step yet)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    \n",
    "    #Configure the diagram \n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           title=\"Receiver operating characteristic example\")\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def train_NN_cv(data, year):\n",
    "   \n",
    "#     time_start=time.time()\n",
    "#     #prepare config list\n",
    "#     conf_list = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "#     #conf_list = [(2,2,),(3,3,),(5,5,),(10,10,)] #dont forget to change name!!!!!!!\n",
    "#     #conf_list = [(2,),(5,),(10,),(15,)]\n",
    "#     conf_name = \"alpha\"\n",
    "#     para = dict()\n",
    "    \n",
    "#     #Get dummy value\n",
    "    \n",
    "#     data['FIRST_TIME_HOMEBUYER_FLAG'] = data['FIRST_TIME_HOMEBUYER_FLAG'].apply(lambda x : np.NaN if x == '9' else x)\n",
    "#     data['NUMBER_OF_UNITS'] = data['NUMBER_OF_UNITS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "#     data['CHANNEL'] = data['CHANNEL'].apply(lambda x : np.NaN if x == 'T' else x)\n",
    "#     data['PROPERTY_TYPE'] = data['PROPERTY_TYPE'].apply(lambda x : np.NaN if x == '99' else x)\n",
    "#     data['NUMBER_OF_BORROWERS'] = data['NUMBER_OF_BORROWERS'].apply(lambda x : np.NaN if x == 99 else x)\n",
    "    \n",
    "    \n",
    "#     output_array = np.asarray(data['default_flag'].astype(int))\n",
    "#     input_array = np.c_[\n",
    "#         data[['CREDIT_SCORE',#0\n",
    "#               'ORIGINAL_DTI_RATIO',#1\n",
    "#               'ORIGINAL_UPB',#2\n",
    "#               'ORIGINAL_LTV',#3\n",
    "#               'ORIGINAL_LOAN_TERM',#4\n",
    "#               'ORIGINAL_INTEREST_RATE'#5\n",
    "#              ]] \n",
    "#     ]\n",
    "        \n",
    "#     scaler = preprocessing.StandardScaler().fit(input_array)\n",
    "#     input_array_N = scaler.transform(input_array)\n",
    "#     #input_array_N = preprocessing.normalize(input_array, norm='l2')\n",
    "#     X = np.c_[\n",
    "#         input_array_N,\n",
    "#         np.asarray(pd.get_dummies(data['FIRST_TIME_HOMEBUYER_FLAG'])), # N,Y,9 str remove 9\n",
    "#         np.asarray(pd.get_dummies(data['NUMBER_OF_UNITS'])), # 1 2 3 4 99 int remove 99\n",
    "#         np.asarray(pd.get_dummies(data['OCCUPANCY_STATUS'])), # P S I str\n",
    "#         np.asarray(pd.get_dummies(data['CHANNEL'])), # T R C B str remove T\n",
    "#         np.asarray(pd.get_dummies(data['PROPERTY_TYPE'])), # SF PU CO MH CP 99 str remove 99\n",
    "#         np.asarray(pd.get_dummies(data['LOAN_PURPOSE'])), # P N C str\n",
    "#         np.asarray(pd.get_dummies(data['NUMBER_OF_BORROWERS'])) # 2 1 99 int remove 99    \n",
    "#     ]\n",
    "#     y = output_array\n",
    "    \n",
    "#     #devide Flods for cv\n",
    "#     cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=10)\n",
    "    \n",
    "    \n",
    "#     #tpr lists and auc value list\n",
    "    \n",
    "#     auc_all_config = []\n",
    "#     mean_aucs = []\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #For ploting, prepare 500 points from 0-1\n",
    "#     mean_fpr = np.linspace(0, 1, 500)\n",
    "      \n",
    "#     #Loop training for every fold\n",
    "#     for conf in conf_list:\n",
    "#         print(\"Training \"+str(conf)+\"...\")\n",
    "#         tprs = []\n",
    "#         aucs = []\n",
    "#         fig, ax = plt.subplots(figsize=(15, 8)) \n",
    "\n",
    "#         for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "# ##############################################################################################################################      \n",
    "#             classifier = MLPClassifier(\n",
    "#                 hidden_layer_sizes = (10,),\n",
    "#                 alpha = conf,\n",
    "#                 learning_rate_init = 0.001,\n",
    "#                 tol = 1e-05,\n",
    "#                 verbose = True,\n",
    "#                 n_iter_no_change = 10\n",
    "#             )\n",
    "#             para = classifier.get_params(deep=True)\n",
    "# ##############################################################################################################################        \n",
    "#             print(\"Training fold \"+str(i)+\"...\")\n",
    "#             classifier.fit(X[train], y[train])\n",
    "#             # put the curve in ax through 'ax = ax'\n",
    "#             viz = plot_roc_curve(classifier, X[test], y[test],\n",
    "#                                  name='ROC fold {}'.format(i),\n",
    "#                                  alpha=0.3, lw=1, ax=ax)\n",
    "\n",
    "#             #Plot every point (500) form 0-1, similiar to bin\n",
    "#             interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "#             interp_tpr[0] = 0.0\n",
    "#             #Buff the result \n",
    "#             tprs.append(interp_tpr)\n",
    "#             aucs.append(viz.roc_auc)\n",
    "           \n",
    "#         #store the auc value for this conf\n",
    "#         std = np.std(aucs)\n",
    "#         aucs.append(std)\n",
    "#         auc_all_config.append(aucs)\n",
    "#         #Plot chance\n",
    "#         ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "#                 label='Chance', alpha=.8)\n",
    "\n",
    "#         #mean value for each colomn\n",
    "#         mean_tpr = np.mean(tprs, axis=0)\n",
    "#         mean_tpr[-1] = 1.0\n",
    "#         mean_auc = auc(mean_fpr, mean_tpr)\n",
    "       \n",
    "#         #store all mean_auc together\n",
    "#         mean_aucs.append(mean_auc)\n",
    "\n",
    "#         std_auc = np.std(aucs)\n",
    "#         #Plot mean\n",
    "#         ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "#                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "#                 lw=2, alpha=.8)\n",
    "\n",
    "#         #Configure the diagram \n",
    "#         ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "#                title=\"Receiver operating characteristic example\")\n",
    "#         ax.legend(loc=\"lower right\")\n",
    "#         file_name = str(conf)+conf_name+str(year)+'.jpg'\n",
    "#         folder = os.getcwd() + '/NN_cv/'+year+'/'+conf_name\n",
    "#         if not os.path.exists(folder):\n",
    "#             os.makedirs(folder)\n",
    "#         fig.savefig('./NN_cv/'+year+'/'+conf_name+'/'+file_name)\n",
    "        \n",
    "#     txt_head = conf_name+'/'.join(str(x) for x in conf_list)\n",
    "#     file_name_all = './NN_cv/'+year+'/'+conf_name+'/'+str(year)+'all.txt'\n",
    "#     file_name_mean = './NN_cv/'+year+'/'+conf_name+'/'+str(year)+'mean.txt'\n",
    "#     file_name_para = './NN_cv/'+year+'/'+conf_name+'/'+'other_para.json'\n",
    "    \n",
    "#     folder = os.getcwd() + '/NN_cv/'+year+'/'+conf_name\n",
    "\n",
    "#     if not os.path.exists(folder):\n",
    "#         os.makedirs(folder)\n",
    "        \n",
    "#     np.savetxt(file_name_all,auc_all_config,fmt='%.7f',delimiter=',', header=txt_head)\n",
    "#     np.savetxt(file_name_mean,mean_aucs,fmt='%.7f',delimiter=',', header=txt_head)\n",
    "#     with open(file_name_para, 'w') as fp:\n",
    "#         json.dump(para, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_NN_cv(X, y): \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state=13\n",
    "    )\n",
    "\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [{\n",
    "        'hidden_layer_sizes': [(5,),(10,),(15,),(5,5)], \n",
    "        'activation': ['logistic', 'tanh', 'relu'],           \n",
    "        'alpha': [0.1, 0.01, 0.001],\n",
    "        'learning_rate_init':[0.01, 0.001, 0.0001]\n",
    "    }]\n",
    "\n",
    "    print(\"# Tuning hyper-parameters for AUC\")\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier(), \n",
    "        tuned_parameters, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=7,\n",
    "        verbose=1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_NN_diff(X, y, conf):\n",
    "    # train test Random state 13\n",
    "    time_start=time.time()\n",
    "    #prepare config list\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state = 13\n",
    "    )\n",
    "    \n",
    "    \n",
    "    classifier = MLPClassifier(\n",
    "            hidden_layer_sizes = conf,\n",
    "            alpha = 0.001,\n",
    "            learning_rate_init = 0.001,\n",
    "            tol = 1e-05,\n",
    "            verbose = True,\n",
    "            n_iter_no_change = 10\n",
    "            \n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "    time_end=time.time()\n",
    "    print('Training done, time cost:',time_end-time_start,'s')\n",
    "    \n",
    "    return classifier   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_NN(X, y, year):\n",
    "    # {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
    "    # train test Random state 13\n",
    "    time_start=time.time()\n",
    "    #prepare config list\n",
    "    para = dict()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state = 13\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))    \n",
    "    \n",
    "    classifier = MLPClassifier(\n",
    "            hidden_layer_sizes = (5,),\n",
    "            activation = 'tanh',\n",
    "            alpha = 0.001,\n",
    "            learning_rate_init = 0.001,\n",
    "            tol = 1e-04,\n",
    "            verbose = True         \n",
    "    )\n",
    "    \n",
    "    para = classifier.get_params(deep=True)\n",
    "    folder = os.getcwd() + '/NN/'\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "    file_name_para = folder+year+'para.json'    \n",
    "    with open(file_name_para, 'w') as fp:\n",
    "        json.dump(para, fp, indent=4)\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "    viz = plot_roc_curve(\n",
    "        classifier, \n",
    "        X_test, \n",
    "        y_test,\n",
    "        name='Test ROC'.format(0),\n",
    "        alpha=0.5, lw=1, ax=ax\n",
    "    )\n",
    "    \n",
    "    viz_train = plot_roc_curve(\n",
    "        classifier, \n",
    "        X_train, \n",
    "        y_train,\n",
    "        name='Train ROC'.format(1),\n",
    "        alpha=0.5, lw=1, ax=ax\n",
    "    ) \n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "    \n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic example\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    time_end=time.time()\n",
    "    print('Training done, time cost:',time_end-time_start,'s')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(folder+'AUC_{year}_NN.jpg'.format(year=year_)) \n",
    "    \n",
    "    fig_loss, ax_loss = plt.subplots(figsize=(15, 8)) \n",
    "    loss_curve = classifier.loss_curve_\n",
    "    ax_loss.plot(np.arange(1, len(loss_curve)+1), loss_curve )\n",
    "    plt.show()\n",
    "    fig_loss.savefig(folder+'train_curve_{year}_NN.jpg'.format(year=year_)) \n",
    "    return classifier   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest_cv(X, y): \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state=13\n",
    "    )\n",
    "\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [{\n",
    "        'n_estimators': [800, 1200], \n",
    "        'max_depth': [10, 12, 14],               \n",
    "        'min_samples_split': [2, 4]\n",
    "    }]\n",
    "\n",
    "    print(\"# Tuning hyper-parameters for AUC\")\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        RandomForestClassifier(), \n",
    "        tuned_parameters, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=7,\n",
    "        verbose=3\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 800}\n",
    "def train_random_forest(X, y): \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state=13\n",
    "    )\n",
    "    print(\"# Tuning hyper-parameters for AUC\")\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "            n_estimators = 800,\n",
    "            max_depth = 10,\n",
    "            min_samples_split = 2,\n",
    "            n_jobs=6,\n",
    "            verbose = 2\n",
    "        )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 800}\n",
    "def train_RF_diff(X, y, conf): \n",
    "    # Split the dataset in two equal parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        stratify = y,\n",
    "        random_state=13\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "            n_estimators = conf,\n",
    "            max_depth = 10,\n",
    "            min_samples_split = 2,\n",
    "            n_jobs=7,\n",
    "            verbose = 2\n",
    "        )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start year 2007...\n",
      "# Tuning hyper-parameters for AUC\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed: 58.7min\n",
      "[Parallel(n_jobs=7)]: Done 540 out of 540 | elapsed: 75.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.788 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.789 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.787 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.787 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.789 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.788 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.788 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.789 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.788 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.788 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.790 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.788 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.795 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.796 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.795 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.795 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.790 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.794 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.794 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.796 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.796 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.796 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.790 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.791 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.792 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.734 (+/-0.234) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.004) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.791 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.796 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.792 (+/-0.002) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.002) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.793 (+/-0.004) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.002) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.794 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.794 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.795 (+/-0.004) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.795 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.793 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.793 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.793 (+/-0.001) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.792 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94    323320\n",
      "           1       0.59      0.03      0.06     43144\n",
      "\n",
      "    accuracy                           0.88    366464\n",
      "   macro avg       0.74      0.51      0.50    366464\n",
      "weighted avg       0.85      0.88      0.83    366464\n",
      "\n",
      "\n",
      "Start year 2008...\n",
      "# Tuning hyper-parameters for AUC\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=7)]: Done 540 out of 540 | elapsed: 70.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.820 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.822 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.819 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.820 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.821 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.820 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.820 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.821 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.820 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.820 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.823 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.822 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.825 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.829 (+/-0.002) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.825 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.001) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.827 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.829 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.004) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.829 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.827 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.827 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.827 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.825 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.824 (+/-0.004) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.825 (+/-0.002) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.829 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.825 (+/-0.005) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.003) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.761 (+/-0.261) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.827 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.002) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.828 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.828 (+/-0.002) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.829 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.827 (+/-0.003) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.827 (+/-0.004) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.827 (+/-0.002) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.826 (+/-0.002) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96    327036\n",
      "           1       0.55      0.06      0.11     26998\n",
      "\n",
      "    accuracy                           0.92    354034\n",
      "   macro avg       0.74      0.53      0.53    354034\n",
      "weighted avg       0.90      0.92      0.90    354034\n",
      "\n",
      "\n",
      "Start year 2009...\n",
      "# Tuning hyper-parameters for AUC\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed: 75.5min\n",
      "[Parallel(n_jobs=7)]: Done 540 out of 540 | elapsed: 99.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.727 (+/-0.015) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.733 (+/-0.010) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.740 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.723 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.734 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.738 (+/-0.012) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.723 (+/-0.012) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.733 (+/-0.012) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.735 (+/-0.010) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.418 (+/-0.028) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.577 (+/-0.152) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.698 (+/-0.010) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.839 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.841 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.823 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.840 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.841 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.828 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.839 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.841 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.829 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.841 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.829 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.833 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.837 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.843 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.837 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.844 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.834 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.825 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.832 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.832 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.827 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.832 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.834 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.825 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.832 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.834 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.833 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.840 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.840 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.840 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.840 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.840 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.844 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.844 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.844 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.844 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.011) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.834 (+/-0.007) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.831 (+/-0.005) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.829 (+/-0.014) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.833 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.833 (+/-0.006) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.826 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.832 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.834 (+/-0.007) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.833 (+/-0.010) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.840 (+/-0.006) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.840 (+/-0.006) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.839 (+/-0.007) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.841 (+/-0.006) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.840 (+/-0.008) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.006) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.005) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.006) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.008) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.006) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.006) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.842 (+/-0.008) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.007) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.006) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    585803\n",
      "           1       0.86      0.00      0.00      6635\n",
      "\n",
      "    accuracy                           0.99    592438\n",
      "   macro avg       0.92      0.50      0.50    592438\n",
      "weighted avg       0.99      0.99      0.98    592438\n",
      "\n",
      "\n",
      "Start year 2010...\n",
      "# Tuning hyper-parameters for AUC\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed: 54.6min\n",
      "[Parallel(n_jobs=7)]: Done 540 out of 540 | elapsed: 74.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.721 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.725 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.730 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.720 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.725 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.731 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.718 (+/-0.016) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.725 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.730 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.513 (+/-0.206) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.637 (+/-0.121) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.711 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.782 (+/-0.016) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.842 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.799 (+/-0.004) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.841 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.842 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.800 (+/-0.007) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.843 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.845 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.735 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.012) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.821 (+/-0.006) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.847 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.829 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.009) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.833 (+/-0.008) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.012) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.753 (+/-0.005) for {'activation': 'logistic', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.809 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.823 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.810 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.807 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.820 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.820 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.811 (+/-0.005) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.820 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.824 (+/-0.006) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.829 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.842 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.838 (+/-0.007) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.844 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.844 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.844 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.846 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.844 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.849 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.849 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.009) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.806 (+/-0.007) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.824 (+/-0.005) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.801 (+/-0.027) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.809 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.822 (+/-0.007) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.817 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.808 (+/-0.015) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.824 (+/-0.010) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.822 (+/-0.006) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.829 (+/-0.012) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.843 (+/-0.008) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.824 (+/-0.024) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.011) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.012) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.840 (+/-0.012) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.844 (+/-0.011) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.011) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.843 (+/-0.009) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.008) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.010) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.009) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.012) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.847 (+/-0.010) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.845 (+/-0.009) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "0.846 (+/-0.010) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.001}\n",
      "0.842 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5,), 'learning_rate_init': 0.0001}\n",
      "0.847 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.009) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.001}\n",
      "0.844 (+/-0.009) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.0001}\n",
      "0.845 (+/-0.013) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.001}\n",
      "0.844 (+/-0.010) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (15,), 'learning_rate_init': 0.0001}\n",
      "0.848 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.01}\n",
      "0.848 (+/-0.011) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.001}\n",
      "0.844 (+/-0.010) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (5, 5), 'learning_rate_init': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    378795\n",
      "           1       0.00      0.00      0.00      2705\n",
      "\n",
      "    accuracy                           0.99    381500\n",
      "   macro avg       0.50      0.50      0.50    381500\n",
      "weighted avg       0.99      0.99      0.99    381500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    year_list = ['2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "    year_list_2004 = ['2007','2008','2009','2010']\n",
    "    c_list = [0.05, 0.5, 1.0, 5.0, 10.0]\n",
    "    \n",
    "    conf_list = [5, 50, 100, 800]\n",
    "    #model_save_path_NS_NP = './LR_experiments/Penalty_CV/{C}/models/{year}.joblib'\n",
    "    \n",
    "    LR_model_save_path_NS_NP = './LR_experiments/N_Penalty/N_Standardized/models/{year}.joblib'\n",
    "    LR_model_save_path_NS_YP = './LR_experiments/Penalty/N_Standardized/models/{year}.joblib'\n",
    "    \n",
    "    LR_model_save_path_YS_NP = './LR_experiments/N_Penalty/Standardized/models/{year}.joblib'\n",
    "    LR_model_save_path_YS_YP = './LR_experiments/Penalty/Standardized/models/{year}.joblib'\n",
    "    \n",
    "    NN_model_save_path = './NN_models_0.01/models/{year}.joblib'\n",
    "    \n",
    "    RF_model_save_path = './RF_models/models/{year}.joblib'\n",
    "    \n",
    "    NN_Alpha_model_save_path = './NN_experiments/deep/{a}/models/{year}.joblib'\n",
    "    LR_C_model_save_path = './LR_experiments/Penalty_CV/{C}/models/{year}.joblib'\n",
    "    RF_Exp_model_save_path = './RF_experiments/Tree/{conf}/models/{year}.joblib'\n",
    "    \n",
    "    X_path = 'data_train/{year}/input.csv'\n",
    "    y_path = 'data_train/{year}/output.csv'\n",
    "\n",
    "    # Command\n",
    "    mode_list = [\"train_NN_cv\"]\n",
    "    \n",
    "    \n",
    "    for mode in mode_list:\n",
    "        if mode == \"data_process\":\n",
    "            for year_ in year_list:\n",
    "                data_path = 'data/{year}/data'.format(year=year_)\n",
    "                data_path_time = 'data/{year}/data_time'.format(year=year_)\n",
    "                data = load_data(data_path, data_path_time)\n",
    "                data.to_csv('data_flag/{}_flag.csv'.format(year_))\n",
    "\n",
    "        elif mode == \"train_NN_cv\":  \n",
    "            for year_ in year_list_2004:\n",
    "                print(\"Start year {}...\".format(year_))\n",
    "                cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                X = X.to_numpy()\n",
    "                y = np.asarray(y['0'].astype(int))\n",
    "                \n",
    "                train_NN_cv(X, y)\n",
    "\n",
    "        elif mode ==\"train_LR_N_S\": # Train LR without standardize\n",
    "            for year_ in year_list:\n",
    "                penalty_list = [0, 1]\n",
    "                for penalty in penalty_list:\n",
    "                    print(\"Loading data...\")\n",
    "                    data_path = 'data_flag/{year}_flag.csv'.format(year=year_)  \n",
    "                    cols = pd.read_csv(data_path).columns\n",
    "                    data = pd.read_csv(data_path, usecols = cols[1:]) \n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(15, 8))         \n",
    "                    print(str(year_)+\" training start...\")\n",
    "\n",
    "                    model = train_log_N_S(data, fig, ax, penalty)\n",
    "                    if penalty == 1: \n",
    "                        folder = os.getcwd() + '/LR_experiments/Penalty/N_Standardized/models/'\n",
    "                        if not os.path.exists(folder):\n",
    "                            os.makedirs(folder)\n",
    "                        dump(model, LR_model_save_path_NS_YP.format(year=year_))\n",
    "                    else:\n",
    "                        folder = os.getcwd() + '/LR_experiments/N_Penalty/N_Standardized/models/'\n",
    "                        if not os.path.exists(folder):\n",
    "                            os.makedirs(folder)\n",
    "                        dump(model, LR_model_save_path_NS_NP.format(year=year_))\n",
    "\n",
    "        elif mode ==\"train_LR_S\": # Train LR using data after standardizing\n",
    "            for year_ in year_list:\n",
    "                print(\"Loading data...\")\n",
    "\n",
    "                cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                X = X.to_numpy()\n",
    "                y = np.asarray(y['0'].astype(int))\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(15, 8))         \n",
    "                print(str(year_)+\" training start...\")\n",
    "                penalty_list = [0, 1]\n",
    "                for penalty in penalty_list:\n",
    "                    if penalty == 0:\n",
    "                        model = train_log_Y_S(X, y, fig, ax, penalty)\n",
    "                        folder = os.getcwd() + '/LR_experiments/N_Penalty/Standardized/models/'\n",
    "                        if not os.path.exists(folder):\n",
    "                            os.makedirs(folder)\n",
    "                        dump(model, LR_model_save_path_YS_NP.format(year=year_))\n",
    "                    else:\n",
    "                        model = train_log_Y_S(X, y, fig, ax, penalty)\n",
    "                        folder = os.getcwd() + '/LR_experiments/Penalty/Standardized/models/'\n",
    "                        if not os.path.exists(folder):\n",
    "                            os.makedirs(folder)\n",
    "                        dump(model, LR_model_save_path_YS_YP.format(year=year_))\n",
    "\n",
    "        elif mode ==\"train_NN\":\n",
    "            for year_ in year_list:\n",
    "                print(\"Loading data...\")\n",
    "                \n",
    "                cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                X = X.to_numpy()\n",
    "                y = np.asarray(y['0'].astype(int))\n",
    "                        \n",
    "                print(str(year_)+\" training start...\")\n",
    "                model = train_NN(X, y, year_)\n",
    "                \n",
    "                dump(model, NN_model_save_path.format(year=year_))\n",
    "                \n",
    "        elif mode ==\"train_RF\":\n",
    "            for year_ in year_list:\n",
    "                print(\"{} Loading data...\".format(year_))\n",
    "                \n",
    "                cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                X = X.to_numpy()\n",
    "                y = np.asarray(y['0'].astype(int))\n",
    "                \n",
    "                model = train_random_forest(X, y)\n",
    "                dump(model, RF_model_save_path.format(year=year_))\n",
    "                \n",
    "        elif mode ==\"train_RF_cv\":\n",
    "            for year_ in year_list_2004:\n",
    "                print(\"Loading data...\")\n",
    "                \n",
    "                cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                X = X.to_numpy()\n",
    "                y = np.asarray(y['0'].astype(int))\n",
    "                \n",
    "                train_random_forest_cv(X, y)\n",
    "                \n",
    "\n",
    "        elif mode ==\"train_LR_C\":\n",
    "            for year_ in year_list:\n",
    "                for c in c_list:\n",
    "                    print(\"C=\"+str(c)+\"Loading data...\")\n",
    "                    cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                    X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                    cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                    y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                    X = X.to_numpy()\n",
    "                    y = np.asarray(y['0'].astype(int))       \n",
    "                    print(str(year_)+\" training start...\")\n",
    "                    model = train_log_diff(X, y, c)\n",
    "                    folder = os.getcwd() + '/LR_experiments/Penalty_CV/{}/models/'.format(c)\n",
    "                    if not os.path.exists(folder):\n",
    "                        os.makedirs(folder)\n",
    "                    dump(model, LR_C_model_save_path.format(C=c, year=year_))\n",
    "\n",
    "        elif mode ==\"train_NN_alpha\":\n",
    "            for year_ in year_list:\n",
    "                for conf in conf_list:\n",
    "                    print(\"Conf = \"+str(conf)+\" Loading data...\")\n",
    "                    \n",
    "                    cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                    X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                    cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                    y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                    X = X.to_numpy()\n",
    "                    y = np.asarray(y['0'].astype(int))\n",
    "                    \n",
    "                    print(str(year_)+\" training start...\")\n",
    "                    model = train_NN_diff(X, y, conf)\n",
    "                    folder = os.getcwd() + '/NN_experiments/deep/{}/models/'.format(conf)\n",
    "                    if not os.path.exists(folder):\n",
    "                        os.makedirs(folder)\n",
    "                    dump(model, NN_Alpha_model_save_path.format(a=conf, year=year_))\n",
    "                    \n",
    "        elif mode ==\"train_RF_conf\":\n",
    "            for year_ in year_list:\n",
    "                for conf in conf_list:\n",
    "                    print(\"Conf = \"+str(conf)+\" Loading data...\")\n",
    "                    \n",
    "                    cols = pd.read_csv(X_path.format(year=year_)).columns\n",
    "                    X = pd.read_csv(X_path.format(year=year_), usecols = cols[1:])\n",
    "                    cols = pd.read_csv(y_path.format(year=year_)).columns\n",
    "                    y = pd.read_csv(y_path.format(year=year_), usecols = cols[1:]) \n",
    "                    X = X.to_numpy()\n",
    "                    y = np.asarray(y['0'].astype(int))\n",
    "                    \n",
    "                    print(str(year_)+\" training start...\")\n",
    "                    model = train_RF_diff(X, y, conf)\n",
    "                    folder = os.getcwd() + '/RF_experiments/Tree/{}/models/'.format(conf)\n",
    "                    if not os.path.exists(folder):\n",
    "                        os.makedirs(folder)\n",
    "                    dump(model, RF_Exp_model_save_path.format(conf=conf, year=year_))          \n",
    "        \n",
    "        elif mode ==\"Standard\":\n",
    "            data_standardize()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
